# ─── Pretrained Backbone Config ───
# Frozen pretrained ViT backbone + trainable NPM/baseline heads.
# Clean comparison: same features, different uncertainty methods.
# All models sized to ~1.3–1.5M trainable params for fair comparison.

seed: 42
device: "cuda"

# ── Data ──
data:
  dataset: "cifar10"
  root: "./data"
  batch_size: 512         # cached features are tiny → large batches OK
  num_workers: 2          # Colab has 2 CPUs; only used for initial caching
  image_size: 224         # resize CIFAR to match pretrained models
  persistent_workers: false
  prefetch_factor: 2

# ── Backbone ──
backbone:
  name: "deit_tiny_patch16_224"   # 5.7M params, embed_dim=192
  # alternatives: vit_small_patch16_224 (22M, embed_dim=384)
  freeze: true

# ── NPM agents ──
model:
  num_agents: 16
  num_classes: 10
  dropout: 0.2              # ↑ from 0.1 to reduce overfitting
  label_smoothing: 0.1      # cross-entropy label smoothing

# ── Market ──
market:
  initial_capital: 1.0
  capital_lr: 0.1
  capital_decay: 0.9
  normalize_payoffs: true
  bet_temperature: 1.0
  feature_keep_prob: 0.5
  evolution_enabled: true
  evolution_interval: 5    # ↓ from 10: 10 events in 50 epochs for diversity
  kill_fraction: 0.1
  mutation_std: 0.15       # ↑ from 0.1: stronger mutation for short training
  diversity_weight: 0.3
  agent_aux_weight: 0.3
  bet_calibration_weight: 0.2

# ── Training (heads only — fast) ──
training:
  epochs: 50              # heads converge faster on pretrained features
  optimizer: "adamw"
  lr: 1.0e-3              # higher LR for heads (backbone frozen)
  weight_decay: 0.01
  warmup_epochs: 5
  scheduler: "cosine"
  gradient_clip: 1.0
  early_stopping_patience: 10   # restore best val model after patience epochs

# ── Baselines (param-matched to NPM ~1.5M) ──
baselines:
  ensemble:
    num_members: 5
    hidden_dim: 448       # 5 × ~292K = ~1.46M trainable
    num_layers: 2
  mc_dropout:
    hidden_dim: 768       # 3-layer → ~1.34M trainable
    num_layers: 3
    mc_samples: 10
  moe:
    num_experts: 16
    top_k: 4
    expert_hidden_dim: 224  # 16 × 2-layer → ~1.54M trainable

# ── Evaluation ──
evaluation:
  ood_datasets: ["cifar100", "svhn"]
  volatility_eps: 0.05
  volatility_steps: 10
