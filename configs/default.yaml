# ─── Neural Prediction Market — Default Config ───
# All hyperparameters in one place for reproducibility.

seed: 42
device: "cuda"        # "cuda" or "cpu"

# ── Data ──
data:
  dataset: "cifar10"   # cifar10 | cifar100 | svhn
  root: "./data"
  batch_size: 1024       # A100 80GB handles 1024+ easily for CIFAR-32x32
  num_workers: 8
  augmentation: true
  persistent_workers: true   # keep workers alive between epochs
  prefetch_factor: 4         # batches prefetched per worker

# ── Model ──
model:
  # ViT backbone
  image_size: 32
  patch_size: 4
  embed_dim: 256
  depth: 6              # number of Transformer blocks
  num_heads: 8           # heads per block (standard attention)

  # NPM agents
  num_agents: 16         # independent classifier heads (traders)
  num_classes: 10
  dropout: 0.1

# ── Market ──
market:
  # Capital dynamics (log-space, epoch-level, mean-reverting)
  initial_capital: 1.0
  capital_lr: 0.1        # η for S_i ← γ·S_i + η·z_i (epoch-level z-score step)
  capital_decay: 0.9     # γ — decay toward equal weights (0.9 = 10-epoch memory window)
  normalize_payoffs: true # standardise epoch payoffs to z-scores before update

  # Betting
  bet_temperature: 1.0   # temperature for bet sigmoid
  feature_keep_prob: 0.7  # each agent sees random 70% of backbone features (structural diversity)
  normalize_payoffs: true # standardise payoffs to z-scores before capital update

  # Evolutionary selection
  evolution_enabled: true
  evolution_interval: 5  # epochs between selection rounds (give capital time to diverge)
  kill_fraction: 0.1     # bottom α% replaced
  mutation_std: 0.02     # noise added to cloned weights

  # Diversity bonus (anti-herding)
  diversity_weight: 0.3  # λ for KL-diversity regularizer (push agents apart)

  # Per-agent auxiliary loss (breaks gradient symmetry)
  agent_aux_weight: 0.3  # λ_aux — each agent's individual CE loss

# ── Training ──
training:
  epochs: 200
  optimizer: "adamw"
  lr: 3.0e-4
  weight_decay: 0.05
  warmup_epochs: 10
  scheduler: "cosine"
  label_smoothing: 0.0   # we rely on market dynamics instead
  gradient_clip: 1.0

# ── Performance (A100 optimizations) ──
performance:
  amp: true              # automatic mixed precision
  amp_dtype: "bfloat16"  # bf16 native on A100 (no scaler needed, more stable than fp16)
  compile: true          # torch.compile — ~20-40% speedup (PyTorch 2.0+)

# ── Evaluation ──
evaluation:
  ood_datasets: ["cifar100", "svhn"]
  volatility_eps: 0.05     # perturbation magnitude (0.01 was too small for normalized images)
  volatility_steps: 10     # number of perturbation samples

# ── Logging ──
logging:
  log_dir: "./runs"
  log_interval: 50       # steps
  save_interval: 10      # epochs
  tensorboard: true
