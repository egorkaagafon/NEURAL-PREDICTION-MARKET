# â”€â”€â”€ Neural Prediction Market â€” Default Config â”€â”€â”€
# All hyperparameters in one place for reproducibility.

seed: 42
device: "cuda"        # "cuda" or "cpu"

# â”€â”€ Data â”€â”€
data:
  dataset: "cifar10"   # cifar10 | cifar100 | svhn
  root: "./data"
  batch_size: 1024       # A100 80GB handles 1024+ easily for CIFAR-32x32
  num_workers: 8
  augmentation: true
  persistent_workers: true   # keep workers alive between epochs
  prefetch_factor: 4         # batches prefetched per worker

# â”€â”€ Model â”€â”€
model:
  # ViT backbone
  image_size: 32
  patch_size: 4
  embed_dim: 256
  depth: 6              # number of Transformer blocks
  num_heads: 8           # heads per block (standard attention)

  # NPM agents
  num_agents: 16         # independent classifier heads (traders)
  num_classes: 10
  dropout: 0.1

# â”€â”€ Market â”€â”€
market:
  # Capital dynamics (log-space, epoch-level, mean-reverting)
  initial_capital: 1.0
  capital_lr: 0.1        # Î· for S_i â† Î³Â·S_i + Î·Â·z_i (epoch-level z-score step)
  capital_decay: 0.9     # Î³ â€” decay toward equal weights (0.9 = 10-epoch memory window)
  normalize_payoffs: true # standardise epoch payoffs to z-scores before update

  # Betting
  bet_temperature: 1.0   # temperature for bet sigmoid
  feature_keep_prob: 0.5  # each agent sees random 50% of backbone features (was 0.7 â€” too much overlap)
  normalize_payoffs: true # standardise payoffs to z-scores before capital update

  # Evolutionary selection
  evolution_enabled: true
  evolution_interval: 20 # epochs between selection rounds (give capital time to diverge)
  kill_fraction: 0.1     # bottom Î±% replaced
  mutation_std: 0.1      # noise added to cloned weights (was 0.02 â€” too small)

  # Diversity bonus (anti-herding)
  diversity_weight: 0.3  # Î» for KL-diversity regularizer (push agents apart)

  # Per-agent auxiliary loss (breaks gradient symmetry)
  agent_aux_weight: 0.3  # Î»_aux â€” each agent's individual CE loss

  # Bet calibration (trains bets to predict agent correctness)
  bet_calibration_weight: 0.2  # Î»_bet â€” BCE(bet, ğŸ™[correct])

# â”€â”€ Training â”€â”€
training:
  epochs: 200
  optimizer: "adamw"
  lr: 3.0e-4
  weight_decay: 0.05
  warmup_epochs: 10
  scheduler: "cosine"
  label_smoothing: 0.0   # we rely on market dynamics instead
  gradient_clip: 1.0

# â”€â”€ Performance (A100 optimizations) â”€â”€
performance:
  amp: true              # automatic mixed precision
  amp_dtype: "bfloat16"  # bf16 native on A100 (no scaler needed, more stable than fp16)
  compile: true          # torch.compile â€” ~20-40% speedup (PyTorch 2.0+)

# â”€â”€ Evaluation â”€â”€
evaluation:
  ood_datasets: ["cifar100", "svhn"]
  volatility_eps: 0.05     # perturbation magnitude (0.01 was too small for normalized images)
  volatility_steps: 10     # number of perturbation samples

# â”€â”€ Logging â”€â”€
logging:
  log_dir: "./runs"
  log_interval: 50       # steps
  save_interval: 10      # epochs
  tensorboard: true
